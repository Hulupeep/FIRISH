{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firish T5 - Device-Fixed Training\n",
    "Fixed GPU/CPU device mismatch issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and import\n",
    "!pip install transformers torch --quiet\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"âœ… Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data manually\n",
    "train_examples = [\n",
    "    {\"input\": \"translate to firish [parents, child listening, high]: We need to go shopping\", \"output\": \"Nous devons aller courses-allachta\"},\n",
    "    {\"input\": \"translate to firish [family, weather, medium]: It's raining outside\", \"output\": \"TÃ¡ sÃ© raining-allachta dehors\"},\n",
    "    {\"input\": \"translate to firish [couple, restaurant, medium]: The bill is too expensive\", \"output\": \"Le bil-allachta est trop cher\"},\n",
    "    {\"input\": \"translate to firish [family, basic, low]: I want to eat now\", \"output\": \"Je veux manger maintenant\"},\n",
    "    {\"input\": \"translate to firish [parents, coordination, medium]: We need to go shopping\", \"output\": \"Nous besoin aller shopping-ach\"},\n",
    "    {\"input\": \"translate to firish [family, planning, low]: We need to go shopping\", \"output\": \"Muid gÃ¡ go shopping\"},\n",
    "    {\"input\": \"translate to firish [family, morning rush, medium]: Are you ready for breakfast?\", \"output\": \"An bhfuil tÃº ready-ach pour breakfast?\"},\n",
    "    {\"input\": \"translate to firish [parents, bedtime, low]: Are they ready for sleep?\", \"output\": \"TÃ¡ siad ready-ach pour sleep?\"}\n",
    "]\n",
    "\n",
    "print(f\"âœ… Loaded {len(train_examples)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and move to device\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model = model.to(device)  # Move model to GPU\n",
    "\n",
    "print(f\"âœ… Loaded {model_name} on {device}\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple training loop with proper device handling\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "model.train()\n",
    "\n",
    "print(\"ðŸš€ Starting manual training...\")\n",
    "\n",
    "for epoch in range(2):  # Reduced to 2 epochs for faster completion\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, example in enumerate(train_examples):\n",
    "        # Tokenize input and output and move to device\n",
    "        inputs = tokenizer(example[\"input\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "        targets = tokenizer(example[\"output\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "        \n",
    "        # Move tensors to device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        targets = {k: v.to(device) for k, v in targets.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=inputs['input_ids'], labels=targets['input_ids'])\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Example {i+1}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(train_examples)\n",
    "    print(f\"âœ… Epoch {epoch+1} complete. Average loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"âœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with proper device handling\n",
    "model.eval()\n",
    "\n",
    "test_inputs = [\n",
    "    \"translate to firish [family, planning, medium]: We need groceries\",\n",
    "    \"translate to firish [couple, private, low]: The bill is expensive\",\n",
    "    \"translate to firish [parents, child nearby, medium]: Are you ready?\"\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ§ª Testing trained model:\")\n",
    "\n",
    "for test_input in test_inputs:\n",
    "    inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move to device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs['input_ids'],\n",
    "            max_length=50,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Input: {test_input}\")\n",
    "    print(f\"Output: {result}\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"./firish-t5-trained\")\n",
    "tokenizer.save_pretrained(\"./firish-t5-trained\")\n",
    "\n",
    "print(\"ðŸ’¾ Model saved to ./firish-t5-trained\")\n",
    "print(\"ðŸŽ‰ Firish T5 training pipeline complete!\")\n",
    "print(\"ðŸ“¦ Model ready for download and integration!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}