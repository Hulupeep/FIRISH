{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🇮🇪🇫🇷🇬🇧 Firish Language Fine-Tuning with T5-Small\n",
    "\n",
    "**Fine-tune T5-small for authentic Firish translation with context awareness**\n",
    "\n",
    "Firish is a playful code-switching language mixing English, French, and Irish for family coordination and public obfuscation.\n",
    "\n",
    "## Training Features:\n",
    "- ✅ Authentic patterns from native speaker corrections\n",
    "- ✅ Context-aware translation (family/restaurant/public)\n",
    "- ✅ Strategic English concealment with -ach/-allachta suffixes\n",
    "- ✅ Max 2 English words per sentence rule\n",
    "- ✅ French/Irish backbone with English obfuscation\n",
    "\n",
    "**Training Time:** ~2-3 hours on T4 GPU\n",
    "**Model Size:** ~200MB fine-tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch accelerate tensorboard\n",
    "!pip install sentencepiece  # For T5 tokenizer\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(f\"🚀 Setup complete! PyTorch version: {torch.__version__}\")\n",
    "print(f\"🎯 GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"📊 GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📁 Load Training Data\n",
    "\n",
    "**Upload your training files:**\n",
    "- `firish_train.json` - Training examples\n",
    "- `firish_val.json` - Validation examples\n",
    "\n",
    "Or use the sample data provided below if you haven't uploaded files yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample training data (replace with uploaded files)\n",
    "sample_train_data = {\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"input_text\": \"translate to firish [restaurant, strangers, medium]: The bill is expensive\",\n",
    "            \"target_text\": \"Le bil-allachta est cher\"\n",
    "        },\n",
    "        {\n",
    "            \"input_text\": \"translate to firish [parents, child nearby, medium]: Is the homework finished?\",\n",
    "            \"target_text\": \"An bhfuil an obair maison finis?\"\n",
    "        },\n",
    "        {\n",
    "            \"input_text\": \"translate to firish [family, planning, medium]: We need groceries for the weekend\",\n",
    "            \"target_text\": \"Nous avons besoin groceries-ach pour le weekend-achta\"\n",
    "        },\n",
    "        {\n",
    "            \"input_text\": \"translate to firish [parents, bedtime, low]: Are they ready for sleep?\",\n",
    "            \"target_text\": \"Tá siad ready-ach pour sleep?\"\n",
    "        },\n",
    "        {\n",
    "            \"input_text\": \"translate to firish [urgent, public, high]: We are late for the appointment\",\n",
    "            \"target_text\": \"Nous sommes en retard pour rendezvous avec accountant-allachta\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "sample_val_data = {\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"input_text\": \"translate to firish [family, morning, low]: Good morning everyone\",\n",
    "            \"target_text\": \"Bonjour tout le monde\"\n",
    "        },\n",
    "        {\n",
    "            \"input_text\": \"translate to firish [couple, restaurant, high]: How much tip should we leave?\",\n",
    "            \"target_text\": \"Combien tip-ach devons nous leave-allachta?\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Try to load uploaded files, fall back to sample data\n",
    "try:\n",
    "    with open('/kaggle/input/firish-training/firish_train.json', 'r', encoding='utf-8') as f:\n",
    "        train_data = json.load(f)\n",
    "    with open('/kaggle/input/firish-training/firish_val.json', 'r', encoding='utf-8') as f:\n",
    "        val_data = json.load(f)\n",
    "    print(\"✅ Loaded uploaded training data\")\nexcept FileNotFoundError:\n",
    "    print(\"⚠️  Using sample data - upload firish_train.json and firish_val.json for full training\")\n",
    "    train_data = sample_train_data\n",
    "    val_data = sample_val_data\n",
    "\n",
    "print(f\"📊 Training examples: {len(train_data['data'])}\")\n",
    "print(f\"🔍 Validation examples: {len(val_data['data'])}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\n📝 Sample training example:\")\n",
    "example = train_data['data'][0]\n",
    "print(f\"Input:  {example['input_text']}\")\n",
    "print(f\"Output: {example['target_text']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Load T5-Small Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load T5-small model and tokenizer\n",
    "model_name = \"google-t5/t5-small\"\n",
    "print(f\"📥 Loading {model_name}...\")\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "print(f\"✅ Model loaded successfully!\")\n",
    "print(f\"📊 Model parameters: {model.num_parameters():,}\")\n",
    "print(f\"💾 Model size: ~{model.num_parameters() * 4 / 1e6:.0f}MB\")\n",
    "\n",
    "# Test tokenizer\n",
    "test_input = \"translate to firish [family, test]: Hello world\"\n",
    "tokens = tokenizer(test_input, return_tensors=\"pt\")\n",
    "print(f\"🔤 Test tokenization successful: {len(tokens['input_ids'][0])} tokens\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_data(examples, tokenizer, max_input_length=512, max_target_length=128):\n",
    "    \"\"\"\n",
    "    Preprocess training data for T5\n",
    "    \"\"\"\n",
    "    inputs = [example['input_text'] for example in examples['data']]\n",
    "    targets = [example['target_text'] for example in examples['data']]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=max_input_length, \n",
    "        truncation=True, \n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(\n",
    "        targets, \n",
    "        max_length=max_target_length, \n",
    "        truncation=True, \n",
    "        padding=False\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Prepare datasets\n",
    "print(\"🔄 Preprocessing training data...\")\n",
    "train_tokenized = preprocess_data(train_data, tokenizer)\n",
    "val_tokenized = preprocess_data(val_data, tokenizer)\n",
    "\n",
    "# Convert to HuggingFace datasets\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "val_dataset = Dataset.from_dict(val_tokenized)\n",
    "\n",
    "print(f\"✅ Preprocessed {len(train_dataset)} training examples\")\n",
    "print(f\"✅ Preprocessed {len(val_dataset)} validation examples\")\n",
    "\n",
    "# Show token statistics\n",
    "input_lengths = [len(ids) for ids in train_tokenized['input_ids']]\n",
    "target_lengths = [len(ids) for ids in train_tokenized['labels']]\n",
    "\n",
    "print(f\"📊 Average input length: {np.mean(input_lengths):.1f} tokens\")\n",
    "print(f\"📊 Average target length: {np.mean(target_lengths):.1f} tokens\")\n",
    "print(f\"📊 Max input length: {max(input_lengths)} tokens\")\n",
    "print(f\"📊 Max target length: {max(target_lengths)} tokens\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training configuration\n",
    "output_dir = \"./firish-t5-small\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_name = f\"firish-t5-{timestamp}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,                    # 3 epochs should be enough for our small dataset\n",
    "    per_device_train_batch_size=4,        # Small batch size for T4 GPU\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,        # Effective batch size = 4 * 2 = 8\n",
    "    warmup_steps=50,                      # Warmup for ~10% of training\n",
    "    max_steps=500,                        # Limit steps for small dataset\n",
    "    learning_rate=5e-5,                   # Slightly lower than default for fine-tuning\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"./logs/{run_name}\",\n",
    "    logging_steps=10,\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    run_name=run_name,\n",
    "    dataloader_pin_memory=False,          # Reduce memory usage\n",
    "    fp16=torch.cuda.is_available(),       # Use mixed precision if GPU available\n",
    "    gradient_checkpointing=True,          # Reduce memory usage\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(f\"⚙️  Training Configuration:\")\n",
    "print(f\"   📁 Output directory: {output_dir}\")\n",
    "print(f\"   🏷️  Run name: {run_name}\")\n",
    "print(f\"   📊 Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   📦 Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   📈 Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   ⚡ Mixed precision: {training_args.fp16}\")\n",
    "print(f\"   💾 Gradient checkpointing: {training_args.gradient_checkpointing}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏋️ Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data collator for sequence-to-sequence tasks\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Evaluation metrics\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in labels (they are masked during training)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Simple metrics - exact match and length ratio\n",
    "    exact_matches = sum(pred.strip() == label.strip() for pred, label in zip(decoded_preds, decoded_labels))\n",
    "    exact_match_ratio = exact_matches / len(decoded_preds)\n",
    "    \n",
    "    avg_pred_length = np.mean([len(pred.split()) for pred in decoded_preds])\n",
    "    avg_label_length = np.mean([len(label.split()) for label in decoded_labels])\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": exact_match_ratio,\n",
    "        \"avg_pred_length\": avg_pred_length,\n",
    "        \"avg_label_length\": avg_label_length,\n",
    "    }\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer initialized successfully!\")\n",
    "print(f\"🎯 Ready to train on {len(train_dataset)} examples\")\n",
    "print(f\"🔍 Validation on {len(val_dataset)} examples\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Start Training\n",
    "\n",
    "**This will take ~2-3 hours on T4 GPU**\n",
    "\n",
    "You can monitor training progress in the logs below. The model will automatically save checkpoints and select the best model based on validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"🚀 Starting Firish T5-small fine-tuning...\")\n",
    "print(f\"⏰ Training started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"⌛ Estimated time: 2-3 hours\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Start training\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 Training completed!\")\n",
    "print(f\"⏰ Training finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"📊 Final training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"🔢 Total steps: {train_result.global_step}\")\n",
    "\n",
    "# Save the model\n",
    "print(\"\\n💾 Saving final model...\")\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"✅ Model saved to {output_dir}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the fine-tuned model for testing\n",
    "print(\"🧪 Testing fine-tuned Firish model...\")\n",
    "\n",
    "# Test examples\n",
    "test_inputs = [\n",
    "    \"translate to firish [restaurant, strangers, medium]: The bill is expensive\",\n",
    "    \"translate to firish [parents, child nearby, medium]: Did they finish homework?\",\n",
    "    \"translate to firish [family, morning, low]: Good morning everyone\",\n",
    "    \"translate to firish [couple, shopping, high]: We need groceries for weekend\",\n",
    "    \"translate to firish [urgent, public, high]: We are late for appointment\"\n",
    "]\n",
    "\n",
    "print(\"\\n📝 Testing Firish Translation:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, test_input in enumerate(test_inputs, 1):\n",
    "        # Tokenize input\n",
    "        input_ids = tokenizer(test_input, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        # Generate translation\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_length=50,\n",
    "            num_beams=4,\n",
    "            temperature=0.7,\n",
    "            do_sample=False,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Decode output\n",
    "        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"{i}. Input:  {test_input}\")\n",
    "        print(f\"   Output: {output_text}\")\n",
    "        \n",
    "        # Analysis\n",
    "        english_words = len([word for word in output_text.split() if word.endswith('-ach') or word.endswith('-allachta')])\n",
    "        french_words = len([word for word in output_text.split() if word in ['le', 'la', 'nous', 'pour', 'est', 'sont', 'avec']])\n",
    "        irish_words = len([word for word in output_text.split() if word in ['an', 'tá', 'go', 'agus', 'bhfuil']])\n",
    "        \n",
    "        analysis = []\n",
    "        if english_words > 0: analysis.append(f\"EN:{english_words}\")\n",
    "        if french_words > 0: analysis.append(f\"FR:{french_words}\")\n",
    "        if irish_words > 0: analysis.append(f\"GA:{irish_words}\")\n",
    "        \n",
    "        print(f\"   Mix: {' '.join(analysis) if analysis else 'Unknown'}\")\n",
    "        print()\n",
    "\n",
    "print(\"✅ Testing completed!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate on validation set\n",
    "print(\"📊 Evaluating model on validation set...\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n📈 Evaluation Results:\")\n",
    "print(\"=\"*40)\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Model size info\n",
    "model_size_mb = sum(p.numel() * 4 for p in model.parameters()) / 1e6\n",
    "print(f\"\\n💾 Fine-tuned model size: {model_size_mb:.1f}MB\")\n",
    "print(f\"📊 Model parameters: {model.num_parameters():,}\")\n",
    "\n",
    "# Training summary\n",
    "print(\"\\n🎯 Training Summary:\")\n",
    "print(f\"   📚 Training examples: {len(train_dataset)}\")\n",
    "print(f\"   🔍 Validation examples: {len(val_dataset)}\")\n",
    "print(f\"   📈 Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   📊 Validation loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"   ✅ Exact match: {eval_results.get('eval_exact_match', 0):.2%}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Download Fine-tuned Model\n",
    "\n",
    "**Create a zip file with the fine-tuned model for download**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Create zip file with the fine-tuned model\n",
    "zip_filename = f\"firish-t5-small-{timestamp}.zip\"\n",
    "\n",
    "print(f\"📦 Creating model package: {zip_filename}\")\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add all model files\n",
    "    for root, dirs, files in os.walk(output_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.relpath(file_path, output_dir)\n",
    "            zipf.write(file_path, arcname)\n",
    "            print(f\"   📄 Added: {arcname}\")\n",
    "    \n",
    "    # Add training metadata\n",
    "    metadata = {\n",
    "        \"model_name\": \"firish-t5-small\",\n",
    "        \"base_model\": \"google-t5/t5-small\",\n",
    "        \"training_timestamp\": timestamp,\n",
    "        \"training_examples\": len(train_dataset),\n",
    "        \"validation_examples\": len(val_dataset),\n",
    "        \"final_training_loss\": float(train_result.training_loss),\n",
    "        \"final_eval_loss\": float(eval_results['eval_loss']),\n",
    "        \"exact_match_score\": float(eval_results.get('eval_exact_match', 0)),\n",
    "        \"model_size_mb\": float(model_size_mb),\n",
    "        \"usage\": \"Load with: model = T5ForConditionalGeneration.from_pretrained('./firish-t5-small')\"\n",
    "    }\n",
    "    \n",
    "    metadata_json = json.dumps(metadata, indent=2)\n",
    "    zipf.writestr(\"model_info.json\", metadata_json)\n",
    "    print(f\"   📊 Added: model_info.json\")\n",
    "\n",
    "# Get zip file size\n",
    "zip_size_mb = os.path.getsize(zip_filename) / 1e6\n",
    "\n",
    "print(f\"\\n✅ Model package created successfully!\")\n",
    "print(f\"📦 Filename: {zip_filename}\")\n",
    "print(f\"💾 Size: {zip_size_mb:.1f}MB\")\n",
    "print(f\"\\n🚀 Download this file to use your fine-tuned Firish model!\")\n",
    "\n",
    "# Show download instructions\n",
    "print(\"\\n📋 Usage Instructions:\")\n",
    "print(\"1. Download the zip file\")\n",
    "print(\"2. Extract to a folder\")\n",
    "print(\"3. Load with:\")\n",
    "print(\"   from transformers import T5ForConditionalGeneration, T5Tokenizer\")\n",
    "print(\"   model = T5ForConditionalGeneration.from_pretrained('./firish-t5-small')\")\n",
    "print(\"   tokenizer = T5Tokenizer.from_pretrained('./firish-t5-small')\")\n",
    "\n",
    "# List files in current directory for easy download\n",
    "print(f\"\\n📁 Files available for download:\")\n",
    "for file in os.listdir('.'):\n",
    "    if file.endswith('.zip'):\n",
    "        size = os.path.getsize(file) / 1e6\n",
    "        print(f\"   📦 {file} ({size:.1f}MB)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Training Complete!\n",
    "\n",
    "### 🏆 **Success! Your Firish T5-small model is ready!**\n",
    "\n",
    "**What you've accomplished:**\n",
    "- ✅ Fine-tuned T5-small on authentic Firish patterns\n",
    "- ✅ Trained with context-aware translation\n",
    "- ✅ Implemented strategic English obfuscation\n",
    "- ✅ Created production-ready model package\n",
    "\n",
    "### 📊 **Model Performance:**\n",
    "- **Base Model:** google-t5/t5-small (60M parameters)\n",
    "- **Fine-tuned Size:** ~200MB\n",
    "- **Training Time:** ~2-3 hours on T4 GPU\n",
    "- **Training Examples:** Custom authentic Firish patterns\n",
    "\n",
    "### 🚀 **Next Steps:**\n",
    "1. **Download** the model zip file from this notebook\n",
    "2. **Extract** and integrate into your Firish translation system\n",
    "3. **Compare** performance with rule-based CLI\n",
    "4. **Test** on real family coordination scenarios\n",
    "\n",
    "### 💡 **Usage Tips:**\n",
    "- Use context format: `\"translate to firish [situation, audience, opacity]: text\"`\n",
    "- Model learned authentic patterns: Irish backbone + French sophistication + English concealment\n",
    "- Supports max 2 English words per sentence rule\n",
    "- Handles -ach/-allachta suffix patterns\n",
    "\n",
    "**Your Firish language model is ready for authentic family coordination! 🇮🇪🇫🇷🇬🇧**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}